{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf2a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nts: activate svd \n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "import threading\n",
    "from pyproj import Proj \n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import random \n",
    "\n",
    "###### settings\n",
    "trackspath1='/home/sonia/mcms/tracker/1940-2010/era5/out_era5/era5/mcms_era5_1940_2010_tracks.txt'\n",
    "trackspath2='/home/sonia/mcms/tracker/2010-2024/era5/out_era5/era5/FIXEDmcms_era5_2010_2024_tracks.txt'\n",
    "joinyear = 2010 # overlap for the track data\n",
    "\n",
    "use_slp = False # whether to include slp channel\n",
    "use_windmag = False #include wind magnitude channel # NOTE THIS IS 500hPa\n",
    "use_winduv = False # include wind u and v components channels # NOTE THIS IS 500hPa\n",
    "use_temperature = False # temperature at 925hPa \n",
    "use_humidity = True # specific humidity at 500hPa\n",
    "use_topo = False # include topography channel\n",
    "skip_preexisting = False # skip existing datapoints (ensures they have 8 frames)\n",
    "threads = 1 # >1 not implemented\n",
    "\n",
    "# atlantic ocean is regmask['reg_name'].values[109] # so 110 in regmaskoc values\n",
    "# atlantic: 110\n",
    "# pacific: 111\n",
    "reg_id = 110\n",
    "hemi = 'n' # n or s\n",
    "###### \n",
    "\n",
    "if reg_id == 110:\n",
    "    basin = 'atlantic'\n",
    "elif reg_id == 111:\n",
    "    basin = 'pacific'\n",
    "basin = hemi + basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac4d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "regmask = xr.open_dataset('/home/cyclone/regmask_0723_anl.nc')\n",
    "\n",
    "####### make dataframe of all tracks \n",
    "tracks1 = pd.read_csv(trackspath1, sep=' ', header=None, \n",
    "        names=['year', 'month', 'day', 'hour', 'total_hrs', 'unk1', 'unk2', 'unk3', 'unk4', 'unk5', 'unk6', \n",
    "               'z1', 'z2', 'unk7', 'tid', 'sid'])\n",
    "# storms that start before the join year (even if they continue into the join year):\n",
    "sids1 = tracks1[(tracks1['sid']==tracks1['tid']) & (tracks1['year']<joinyear)]['sid'].unique()\n",
    "tracks1 = tracks1[tracks1['sid'].isin(sids1)]\n",
    "\n",
    "tracks2 = pd.read_csv(trackspath2, sep=' ', header=None, \n",
    "        names=['year', 'month', 'day', 'hour', 'total_hrs', 'unk1', 'unk2', 'unk3', 'unk4', 'unk5', 'unk6', \n",
    "               'z1', 'z2', 'unk7', 'tid', 'sid'])\n",
    "# filter out storms that \"start\" at the beginning of the join year since they probably started before and are \n",
    "# included in tracks1\n",
    "sids2 = tracks2[(tracks2['sid']==tracks2['tid']) & \\\n",
    "        ((tracks2['year']>=joinyear) | (tracks2['month']>1) | (tracks2['day']>1) | (tracks2['hour']>0))]['sid'].unique()\n",
    "tracks2 = tracks2[tracks2['sid'].isin(sids2)]\n",
    "\n",
    "tracks = pd.concat([tracks1, tracks2], ignore_index=True)\n",
    "tracks = tracks.sort_values(by=['year', 'month', 'day', 'hour'])\n",
    "\n",
    "# conversions from the MCMS lat/lon system, as described in Jimmy's email:\n",
    "tracks['lat'] = 90-tracks['unk1'].values/100\n",
    "tracks['lon'] = tracks['unk2'].values/100\n",
    "\n",
    "tracks = tracks[['year', 'month', 'day', 'hour', 'tid', 'sid', 'lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edf28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### variables prep\n",
    "grid = 0.25\n",
    "varnames = [] # list of variables that will be included in this output dataset\n",
    "varlocs = {'slp': f'/mnt/data/sonia/cyclone/{grid}/slp', #'wind10m': '/home/cyclone/wind',\n",
    "           'wind': f'/mnt/data/sonia/cyclone/{grid}/wind_500hpa',\n",
    "           'temperature': f'/mnt/data/sonia/cyclone/{grid}/temperature',\n",
    "           'humidity': f'/mnt/data/sonia/cyclone/{grid}/humidity',\n",
    "           'topo': f'/mnt/data/sonia/cyclone/{grid}/slp/topo.nc'} # where the source data is stored \n",
    "varfuncs = {}\n",
    "climatology = {}\n",
    "if use_slp:\n",
    "    varnames.append('slp')\n",
    "    def f_slp(ds, lats, lons, time=None): # function to run when new SLP file is loaded\n",
    "        if time is None:\n",
    "            return ds.sel(lat=lats, lon=lons)['slp']\n",
    "        else:\n",
    "            return ds.sel(lat=lats, lon=lons, time=time)['slp']\n",
    "    varfuncs['slp'] = f_slp\n",
    "if use_windmag:\n",
    "    varnames.append('wind')\n",
    "    def f_wind(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            u = ds.sel(lat=lats, lon=lons)['u'] # for 10m: [['u10', 'v10']] \n",
    "            v = ds.sel(lat=lats, lon=lons)['v']\n",
    "        else:\n",
    "            u = ds.sel(lat=lats, lon=lons, time=time)['u'] # for 10m: [['u10', 'v10']] \n",
    "            v = ds.sel(lat=lats, lon=lons, time=time)['v']\n",
    "        windmag = np.sqrt(u**2 + v**2)\n",
    "        return windmag\n",
    "    varfuncs['wind'] = f_wind\n",
    "if use_winduv:\n",
    "    varnames.append('wind')\n",
    "    def f_winduv(ds, lats, lons, time=None):\n",
    "        # print(ds.sel(lat=lats, lon=lons))\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons)[['u', 'v']] # for 10m: [['u10', 'v10']] \n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time)[['u', 'v']] # for 10m: [['u10', 'v10']] \n",
    "        return data\n",
    "    varfuncs['wind'] = f_winduv\n",
    "if use_temperature:\n",
    "    varnames.append('temperature')\n",
    "    def f_temperature(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons, pressure_level=925)['t']\n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time, pressure_level=925)['t']\n",
    "        return data\n",
    "    varfuncs['temperature'] = f_temperature\n",
    "if use_humidity:\n",
    "    varnames.append('humidity')\n",
    "    def f_humidity(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons, pressure_level=500)['q']\n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time, pressure_level=500)['q']\n",
    "        return data \n",
    "    varfuncs['humidity'] = f_humidity\n",
    "topo = None\n",
    "if use_topo: \n",
    "    varnames.append('topo')\n",
    "    topo = xr.open_dataset(varlocs['topo'], engine='netcdf4')\n",
    "    def f_topo(ds, lats, lons, time=None):\n",
    "        return ds.sel(lat=lats, lon=lons)['lsm']\n",
    "varnames, varfuncs\n",
    "\n",
    "resolution = grid # resolution of data in degs (may later get redefined by climax checkpoint reso)\n",
    "l = 800 # (half length: l/2 km from center in each direction)\n",
    "s = 32 # box will be dimensions s by s (eg 32x32)\n",
    "x_lin = np.linspace(-l, l, s)\n",
    "y_lin = np.linspace(-l, l, s)\n",
    "x_grid, y_grid = np.meshgrid(x_lin, y_lin) # equal-spaced points from -l to l in both x and y dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76aea4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_170815/1857287072.py:6: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  correct_time = cur_data['time'].values[0] + pd.to_timedelta(np.arange(cur_data.dims['time']) * 6, unit='h')\n"
     ]
    }
   ],
   "source": [
    "file_year = 1940\n",
    "end_year = 2024\n",
    "cur_datas = {}\n",
    "for var in varnames:\n",
    "    cur_data = xr.open_dataset(f'{varlocs[var]}/{var}.{file_year}.nc', engine='netcdf4')\n",
    "    correct_time = cur_data['time'].values[0] + pd.to_timedelta(np.arange(cur_data.dims['time']) * 6, unit='h')\n",
    "    cur_data = cur_data.assign_coords(time=correct_time) # incase it wasn't read in as 6hrly\n",
    "    cur_datas[var] = cur_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdccd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "truetrain = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/train'))\n",
    "trueval = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/val'))\n",
    "truetest = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/test'))\n",
    "\n",
    "tracks['split'] = 0\n",
    "tracks.loc[tracks['sid'].isin(trueval), 'split'] = 1\n",
    "tracks.loc[tracks['sid'].isin(truetest), 'split'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bf75a",
   "metadata": {},
   "source": [
    "# The inverse\n",
    "From global climaX output to series of 32x32 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181dd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj \n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "predpath = '/mnt/data/sonia/aurora-out/date/raw-natlantic-multivar-fullcontext/test'\n",
    "outpath = '/mnt/data/sonia/aurora-out/date/patches-natlantic-multivar-fullcontext/test'\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "if 'climax' in predpath:\n",
    "    resolution = 1.40625\n",
    "elif 'aurora' in predpath:\n",
    "    resolution = 0.5\n",
    "l = 800 # (half length: l/2 km from center in each direction)\n",
    "s = 32 # box will be dimensions s by s (eg 32x32)\n",
    "### HERE WE REINTRODUE THE N/S FLIP: ###\n",
    "x_lin = np.linspace(-l, l, s)\n",
    "y_lin = np.linspace(-l, l, s)\n",
    "x_grid, y_grid = np.meshgrid(x_lin, y_lin) # equal-spaced points from -l to l in both x and y dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12a7b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/439 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1440, 5)\n",
      "<xarray.DataArray 'slp' (lat: 720, lon: 1440)> Size: 4MB\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]],\n",
      "      shape=(720, 1440), dtype=float32)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 6kB -90.0 -89.75 -89.5 -89.25 ... 89.5 89.75 90.0\n",
      "  * lon      (lon) float64 12kB 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n",
      "(720, 1440, 5)\n",
      "<xarray.DataArray 'slp' (lat: 720, lon: 1440)> Size: 4MB\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]],\n",
      "      shape=(720, 1440), dtype=float32)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 6kB -90.0 -89.75 -89.5 -89.25 ... 89.5 89.75 90.0\n",
      "  * lon      (lon) float64 12kB 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All-NaN slice encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     53\u001b[39m z_search = ds[key].sel(\n\u001b[32m     54\u001b[39m     lat=\u001b[38;5;28mslice\u001b[39m(lat_center - search_radius, lat_center + search_radius),\n\u001b[32m     55\u001b[39m     lon=\u001b[38;5;28mslice\u001b[39m(lon_center - search_radius, lon_center + search_radius)\n\u001b[32m     56\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m z_search.size > \u001b[32m0\u001b[39m: \n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     min_idx = np.unravel_index(\u001b[43mz_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.values, z_search.shape)\n\u001b[32m     59\u001b[39m     lat_center = z_search.lat[min_idx[\u001b[32m0\u001b[39m]].values.item()\n\u001b[32m     60\u001b[39m     lon_center = z_search.lon[min_idx[\u001b[32m1\u001b[39m]].values.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/dataarray.py:6208\u001b[39m, in \u001b[36mDataArray.argmin\u001b[39m\u001b[34m(self, dim, axis, keep_attrs, skipna)\u001b[39m\n\u001b[32m   6113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34margmin\u001b[39m(\n\u001b[32m   6114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   6115\u001b[39m     dim: Dims = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   6119\u001b[39m     skipna: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   6120\u001b[39m ) -> Self | \u001b[38;5;28mdict\u001b[39m[Hashable, Self]:\n\u001b[32m   6121\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Index or indices of the minimum of the DataArray over one or more dimensions.\u001b[39;00m\n\u001b[32m   6122\u001b[39m \n\u001b[32m   6123\u001b[39m \u001b[33;03m    If a sequence is passed to 'dim', then result returned as dict of DataArrays,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   6206\u001b[39m \u001b[33;03m    Dimensions without coordinates: y\u001b[39;00m\n\u001b[32m   6207\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6208\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   6210\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m._replace_maybe_drop_dims(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/variable.py:2494\u001b[39m, in \u001b[36mVariable.argmin\u001b[39m\u001b[34m(self, dim, axis, keep_attrs, skipna)\u001b[39m\n\u001b[32m   2451\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34margmin\u001b[39m(\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2453\u001b[39m     dim: Dims = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2456\u001b[39m     skipna: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2457\u001b[39m ) -> Variable | \u001b[38;5;28mdict\u001b[39m[Hashable, Variable]:\n\u001b[32m   2458\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Index or indices of the minimum of the Variable over one or more dimensions.\u001b[39;00m\n\u001b[32m   2459\u001b[39m \u001b[33;03m    If a sequence is passed to 'dim', then result returned as dict of Variables,\u001b[39;00m\n\u001b[32m   2460\u001b[39m \u001b[33;03m    which can be passed directly to isel(). If a single str is passed to 'dim' then\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2492\u001b[39m \u001b[33;03m    DataArray.argmin, DataArray.idxmin\u001b[39;00m\n\u001b[32m   2493\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_unravel_argminmax\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43margmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/variable.py:2415\u001b[39m, in \u001b[36mVariable._unravel_argminmax\u001b[39m\u001b[34m(self, argminmax, dim, axis, keep_attrs, skipna)\u001b[39m\n\u001b[32m   2406\u001b[39m     dim = \u001b[38;5;28mself\u001b[39m.dims\n\u001b[32m   2407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2408\u001b[39m     dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2409\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2413\u001b[39m     \u001b[38;5;66;03m# Return int index if single dimension is passed, and is not part of a\u001b[39;00m\n\u001b[32m   2414\u001b[39m     \u001b[38;5;66;03m# sequence\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m        \u001b[49m\u001b[43margminmax_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\n\u001b[32m   2417\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[38;5;66;03m# Get a name for the new dimension that does not conflict with any existing\u001b[39;00m\n\u001b[32m   2420\u001b[39m \u001b[38;5;66;03m# dimension\u001b[39;00m\n\u001b[32m   2421\u001b[39m newdimname = \u001b[33m\"\u001b[39m\u001b[33m_unravel_argminmax_dim_0\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/variable.py:1681\u001b[39m, in \u001b[36mVariable.reduce\u001b[39m\u001b[34m(self, func, dim, axis, keep_attrs, keepdims, **kwargs)\u001b[39m\n\u001b[32m   1674\u001b[39m keep_attrs_ = (\n\u001b[32m   1675\u001b[39m     _get_keep_attrs(default=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m keep_attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m keep_attrs\n\u001b[32m   1676\u001b[39m )\n\u001b[32m   1678\u001b[39m \u001b[38;5;66;03m# Note that the call order for Variable.mean is\u001b[39;00m\n\u001b[32m   1679\u001b[39m \u001b[38;5;66;03m#    Variable.mean -> NamedArray.mean -> Variable.reduce\u001b[39;00m\n\u001b[32m   1680\u001b[39m \u001b[38;5;66;03m#    -> NamedArray.reduce\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1681\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[38;5;66;03m# return Variable always to support IndexVariable\u001b[39;00m\n\u001b[32m   1686\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Variable(\n\u001b[32m   1687\u001b[39m     result.dims, result._data, attrs=result._attrs \u001b[38;5;28;01mif\u001b[39;00m keep_attrs_ \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1688\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/namedarray/core.py:919\u001b[39m, in \u001b[36mNamedArray.reduce\u001b[39m\u001b[34m(self, func, dim, axis, keepdims, **kwargs)\u001b[39m\n\u001b[32m    917\u001b[39m         data = func(\u001b[38;5;28mself\u001b[39m.data, axis=axis, **kwargs)\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m         data = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m, ()) == \u001b[38;5;28mself\u001b[39m.shape:\n\u001b[32m    922\u001b[39m     dims = \u001b[38;5;28mself\u001b[39m.dims\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/duck_array_ops.py:531\u001b[39m, in \u001b[36m_create_nan_agg_method.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwargs)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n\u001b[32m    530\u001b[39m         warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAll-NaN slice encountered\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_duck_dask_array(values):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/computation/nanops.py:86\u001b[39m, in \u001b[36mnanargmin\u001b[39m\u001b[34m(a, axis)\u001b[39m\n\u001b[32m     83\u001b[39m     fill_value = dtypes.get_pos_infinity(a.dtype)\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _nan_argminmax_object(\u001b[33m\"\u001b[39m\u001b[33margmin\u001b[39m\u001b[33m\"\u001b[39m, fill_value, a, axis=axis)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnputils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanargmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/nputils.py:242\u001b[39m, in \u001b[36m_create_method.<locals>.f\u001b[39m\u001b[34m(values, axis, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m         result = np.float64(result)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnpmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:562\u001b[39m, in \u001b[36mnanargmin\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m    560\u001b[39m     mask = np.all(mask, axis=axis)\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.any(mask):\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll-NaN slice encountered\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    563\u001b[39m res = np.argmin(a, axis=axis, out=out, keepdims=keepdims)\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[31mValueError\u001b[39m: All-NaN slice encountered"
     ]
    }
   ],
   "source": [
    "\n",
    "for fname in tqdm(os.listdir(predpath)):\n",
    "    if 'climax' in predpath:\n",
    "        worlds = np.load(os.path.join(predpath, fname))\n",
    "        if np.isnan(worlds).sum() > 0:\n",
    "            print('NANS', np.isnan(worlds).sum())\n",
    "        sid = fname[:-4]\n",
    "    elif 'aurora' in predpath:\n",
    "        worlds = [] \n",
    "        for tfile in sorted(os.listdir(os.path.join(predpath, fname))):\n",
    "            worlds.append(np.load(os.path.join(predpath, fname, tfile)))\n",
    "        sid = fname\n",
    "    records = tracks[tracks['sid']==sid].to_dict('records')\n",
    "    boxes = []\n",
    "    \n",
    "    track = []\n",
    "        \n",
    "    for t, (record, world) in enumerate(zip(records, worlds)): # iterates over time\n",
    "        if 'climax' in predpath:\n",
    "            lats = np.linspace(90, -90, 128)\n",
    "            lons = np.linspace(0, 360, 256, endpoint=False)\n",
    "            data_vars={'t2m': (('lat', 'lon'), world[0]),\n",
    "                       'z': (('lat', 'lon'), world[1]),\n",
    "                       'u': (('lat', 'lon'), world[2]),\n",
    "                       'v': (('lat', 'lon'), world[3]),\n",
    "                       't': (('lat', 'lon'), world[4]),\n",
    "                       'q': (('lat', 'lon'), world[5])}\n",
    "        elif 'aurora' in predpath:\n",
    "            print(world.shape)\n",
    "            lats = np.linspace(90, -90, 720)\n",
    "            lons = np.linspace(0, 360, 1440, endpoint=False)\n",
    "            data_vars={'slp': (('lat', 'lon'), world[:,:,0]),\n",
    "                       'u': (('lat', 'lon'), world[:,:,1]),\n",
    "                       'v': (('lat', 'lon'), world[:,:,2]),\n",
    "                       't': (('lat', 'lon'), world[:,:,3]),\n",
    "                       'q': (('lat', 'lon'), world[:,:,4])}\n",
    "        \n",
    "        ds = xr.Dataset(\n",
    "            data_vars=data_vars, \n",
    "            coords={\"lat\": lats, \"lon\": lons}, # Attach the coordinates\n",
    "        )\n",
    "        \n",
    "        # Flip strictly to make coordinates ascending for RegularGridInterpolator\n",
    "        ds = ds.reindex(lat=ds.lat[::-1])\n",
    "        \n",
    "        # LAZY TRACKER: Track SLP minimum using Geopotential (z) as a physical proxy\n",
    "        if 'climax' in predpath: key='z'\n",
    "        elif 'aurora' in predpath: key='slp'\n",
    "        print(ds[key])\n",
    "        if t == 0:\n",
    "            lat_center, lon_center = record['lat'], record['lon'] % 360\n",
    "        else:\n",
    "            search_radius = 10.0\n",
    "            z_search = ds[key].sel(\n",
    "                lat=slice(lat_center - search_radius, lat_center + search_radius),\n",
    "                lon=slice(lon_center - search_radius, lon_center + search_radius)\n",
    "            )\n",
    "            if z_search.size > 0: \n",
    "                min_idx = np.unravel_index(z_search.argmin().values, z_search.shape)\n",
    "                lat_center = z_search.lat[min_idx[0]].values.item()\n",
    "                lon_center = z_search.lon[min_idx[1]].values.item()\n",
    "                \n",
    "        track.append((lat_center, lon_center))\n",
    "\n",
    "        proj_km = Proj(proj='aeqd', lat_0=lat_center, lon_0=lon_center, units='km')\n",
    "        lon_grid, lat_grid = proj_km(x_grid, y_grid, inverse=True) #translate km to deg\n",
    "        lon_grid=(lon_grid+360)%360 # because these datasets have lon as 0 to 360 (lat is still -90 to 90)\n",
    "        lon_min = lon_grid.min() - resolution # +- reso because otherwise xarray will not include the edge points\n",
    "        lon_max = lon_grid.max() + resolution\n",
    "        lat_min = lat_grid.min() - resolution\n",
    "        lat_max = lat_grid.max() + resolution\n",
    "\n",
    "        selection = ds.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))\n",
    "        arealats = selection.lat.values \n",
    "        arealons = selection.lon.values\n",
    "        data = selection.to_array().values\n",
    "        data_transposed = np.transpose(data, (1, 2, 0)) # now H W V\n",
    "        \n",
    "        interp = RegularGridInterpolator(\n",
    "            (arealats, arealons),\n",
    "            data_transposed,\n",
    "            bounds_error=False,\n",
    "            fill_value=None\n",
    "        )\n",
    "\n",
    "        # Interpolate at new (lat, lon) pairs\n",
    "        interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "        interp_values = interp(interp_points).reshape(s, s, data.shape[0])\n",
    "        \n",
    "        if 'climax' in outpath:\n",
    "            t2m_slice = interp_values[:, :, 0]\n",
    "            z_slice = interp_values[:, :, 1]\n",
    "            u_slice = interp_values[:, :, 2]\n",
    "            v_slice = interp_values[:, :, 3]\n",
    "            t_slice = interp_values[:, :, 4]\n",
    "            q_slice = interp_values[:, :, 5]\n",
    "                \n",
    "            # Formula: P_slp = P_level * exp(Phi / (Rd * T))\n",
    "            slp = 925 * np.exp(z_slice / (287.05 * t2m_slice))\n",
    "        elif 'aurora' in outpath:\n",
    "            slp_slice = interp_values[:, :, 0]\n",
    "            u_slice = interp_values[:, :, 1]\n",
    "            v_slice = interp_values[:, :, 2]\n",
    "            t_slice = interp_values[:, :, 3]\n",
    "            q_slice = interp_values[:, :, 4]\n",
    "        \n",
    "        frame = np.stack([slp_slice, u_slice, v_slice, t_slice, q_slice], axis=-1) # We want H x W x V\n",
    "        boxes.append(frame)\n",
    "\n",
    "    # result = np.stack(boxes, axis=0)\n",
    "    # np.save(os.path.join(outpath, f'{sid}.npy'), result)\n",
    "    os.makedirs(os.path.join(outpath, sid), exist_ok=True)\n",
    "    for i in range(len(boxes)):\n",
    "        np.save(os.path.join(outpath, sid, f'{i}.npy'), boxes[i])\n",
    "    with open(os.path.join(outpath, sid, 'track.csv'), 'w') as f:\n",
    "        f.write('t,lat,lon\\n')\n",
    "        for t, (lat, lon) in enumerate(track):\n",
    "            f.write(f'{t},{lat},{lon}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c83631f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5eeed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6, 128, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worlds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208d938",
   "metadata": {},
   "source": [
    "# Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "154cbabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/sonia/clima_patches/date/natlantic-humidity-0.25'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = f'/mnt/data/sonia/clima_patches/date/{basin}-humidity-0.25'\n",
    "climatology_path = '/mnt/data/sonia/cyclone/0.25/humidity/monthly_climatology.nc'\n",
    "clima = xr.open_dataset(climatology_path)\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "readme = f'based on climatology at {climatology_path}'\n",
    "with open(os.path.join(out_path, 'readme.txt'), 'w') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "testvalsids = tracks[tracks['split']>0]\n",
    "testvalframes = [group.to_dict('records') for _, group in testvalsids.groupby('sid')]\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33e9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_point_fulldata(frames): # provide actual climate data, not patch plus climatology\n",
    "    \"\"\"make one training datapoint. df contains year/../hr, lat, lon of center\"\"\"\n",
    "    # if skip_preexisting and frame['sid'] in exists:\n",
    "    #     return\n",
    "    boxes = []\n",
    "            \n",
    "    for frame in frames:\n",
    "        year, month, day, hour = frame['year'], frame['month'], frame['day'], frame['hour']\n",
    "        time = f'{year}-{month:02d}-{day:02d}T{hour:02d}:00:00'\n",
    "        cur_datas = clima.sel(month=month)\n",
    "        \n",
    "        lat_center, lon_center = frame['lat'], frame['lon']\n",
    "        # 'aeqd': https://proj.org/en/stable/operations/projections/aeqd.html\n",
    "        proj_km = Proj(proj='aeqd', lat_0=lat_center, lon_0=lon_center, units='km')\n",
    "        # Project to find lat/lon corners of the equal-area box\n",
    "        lon_grid, lat_grid = proj_km(x_grid, y_grid, inverse=True) #translate km to deg\n",
    "        lon_grid=(lon_grid+360)%360 # because these datasets have lon as 0 to 360 (lat is still -90 to 90)\n",
    "        lon_min = lon_grid.min() - resolution # +- reso because otherwise xarray will not include the edge points\n",
    "        lon_max = lon_grid.max() + resolution\n",
    "        lat_min = lat_grid.min() - resolution\n",
    "        lat_max = lat_grid.max() + resolution\n",
    "        \n",
    "        # for var in varnames:\n",
    "        data = varfuncs[var](cur_datas, slice(lat_max, lat_min), slice(lon_min, lon_max), time=None)\n",
    "        \n",
    "        lats = data.lat.values \n",
    "        lons = data.lon.values\n",
    "        \n",
    "        slices=[]\n",
    "        if type(data) == xr.Dataset: # for instance, wind u and v components (data.shape[0] or data.to_array().shape[0] ??)\n",
    "            data = data.to_array().values.squeeze()\n",
    "            # print(data.shape)\n",
    "            for i in range(data.shape[0]):\n",
    "                sel = data[i]\n",
    "                # Build interpolator\n",
    "                interp = RegularGridInterpolator(\n",
    "                    (lats, lons),\n",
    "                    sel,\n",
    "                    bounds_error=False,\n",
    "                    fill_value=np.nan\n",
    "                )\n",
    "                \n",
    "                # Interpolate at new (lat, lon) pairs\n",
    "                interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "                interp_values = interp(interp_points).reshape(s, s)\n",
    "                slices.append(interp_values)\n",
    "        else: # just one channel (eg slp)\n",
    "            data = np.asarray(data).squeeze()\n",
    "            # Build interpolator\n",
    "            interp = RegularGridInterpolator(\n",
    "                (lats, lons),\n",
    "                data,\n",
    "                bounds_error=False,\n",
    "                fill_value=np.nan\n",
    "            )\n",
    "            \n",
    "            # Interpolate at new (lat, lon) pairs\n",
    "            interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "            interp_values = interp(interp_points).reshape(s, s)\n",
    "            slices.append(interp_values)\n",
    "            \n",
    "        boxes.append(np.stack(slices, axis=-1).squeeze())\n",
    "        \n",
    "    # split = '' \n",
    "    if frame['split'] == 0:\n",
    "        split = 'train'\n",
    "    elif frame['split'] == 1:\n",
    "        split = 'val'\n",
    "    elif frame['split'] == 2:\n",
    "        split = 'test'\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected split value {frame['split']}\")\n",
    "    \n",
    "    os.makedirs(os.path.join(out_path, split, frame['sid']), exist_ok=True)\n",
    "    for i in range(len(boxes)):\n",
    "        np.save(os.path.join(out_path, split, f\"{frame['sid']}/{i}\"), boxes[i])\n",
    "        \n",
    "    return boxes\n",
    "    \n",
    "# prompt = prep_point(tracks.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbffffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 899/899 [00:08<00:00, 109.94it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(out_path, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_path, 'test'), exist_ok=True)\n",
    "\n",
    "for storm in tqdm(testvalframes):\n",
    "    boxes =prep_point_fulldata(storm[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5a7f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ae908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
