{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf2a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nts: activate svd \n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "import threading\n",
    "from pyproj import Proj \n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import random \n",
    "\n",
    "###### settings\n",
    "trackspath1='/home/sonia/mcms/tracker/1940-2010/era5/out_era5/era5/mcms_era5_1940_2010_tracks.txt'\n",
    "trackspath2='/home/sonia/mcms/tracker/2010-2024/era5/out_era5/era5/FIXEDmcms_era5_2010_2024_tracks.txt'\n",
    "joinyear = 2010 # overlap for the track data\n",
    "\n",
    "use_slp = False # whether to include slp channel\n",
    "use_windmag = False #include wind magnitude channel # NOTE THIS IS 500hPa\n",
    "use_winduv = False # include wind u and v components channels # NOTE THIS IS 500hPa\n",
    "use_temperature = False # temperature at 925hPa \n",
    "use_humidity = True # specific humidity at 500hPa\n",
    "use_topo = False # include topography channel\n",
    "skip_preexisting = False # skip existing datapoints (ensures they have 8 frames)\n",
    "threads = 1 # >1 not implemented\n",
    "\n",
    "# atlantic ocean is regmask['reg_name'].values[109] # so 110 in regmaskoc values\n",
    "# atlantic: 110\n",
    "# pacific: 111\n",
    "reg_id = 110\n",
    "hemi = 'n' # n or s\n",
    "###### \n",
    "\n",
    "if reg_id == 110:\n",
    "    basin = 'atlantic'\n",
    "elif reg_id == 111:\n",
    "    basin = 'pacific'\n",
    "basin = hemi + basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac4d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "regmask = xr.open_dataset('/home/cyclone/regmask_0723_anl.nc')\n",
    "\n",
    "####### make dataframe of all tracks \n",
    "tracks1 = pd.read_csv(trackspath1, sep=' ', header=None, \n",
    "        names=['year', 'month', 'day', 'hour', 'total_hrs', 'unk1', 'unk2', 'unk3', 'unk4', 'unk5', 'unk6', \n",
    "               'z1', 'z2', 'unk7', 'tid', 'sid'])\n",
    "# storms that start before the join year (even if they continue into the join year):\n",
    "sids1 = tracks1[(tracks1['sid']==tracks1['tid']) & (tracks1['year']<joinyear)]['sid'].unique()\n",
    "tracks1 = tracks1[tracks1['sid'].isin(sids1)]\n",
    "\n",
    "tracks2 = pd.read_csv(trackspath2, sep=' ', header=None, \n",
    "        names=['year', 'month', 'day', 'hour', 'total_hrs', 'unk1', 'unk2', 'unk3', 'unk4', 'unk5', 'unk6', \n",
    "               'z1', 'z2', 'unk7', 'tid', 'sid'])\n",
    "# filter out storms that \"start\" at the beginning of the join year since they probably started before and are \n",
    "# included in tracks1\n",
    "sids2 = tracks2[(tracks2['sid']==tracks2['tid']) & \\\n",
    "        ((tracks2['year']>=joinyear) | (tracks2['month']>1) | (tracks2['day']>1) | (tracks2['hour']>0))]['sid'].unique()\n",
    "tracks2 = tracks2[tracks2['sid'].isin(sids2)]\n",
    "\n",
    "tracks = pd.concat([tracks1, tracks2], ignore_index=True)\n",
    "tracks = tracks.sort_values(by=['year', 'month', 'day', 'hour'])\n",
    "\n",
    "# conversions from the MCMS lat/lon system, as described in Jimmy's email:\n",
    "tracks['lat'] = 90-tracks['unk1'].values/100\n",
    "tracks['lon'] = tracks['unk2'].values/100\n",
    "\n",
    "tracks = tracks[['year', 'month', 'day', 'hour', 'tid', 'sid', 'lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edf28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### variables prep\n",
    "grid = 0.25\n",
    "varnames = [] # list of variables that will be included in this output dataset\n",
    "varlocs = {'slp': f'/mnt/data/sonia/cyclone/{grid}/slp', #'wind10m': '/home/cyclone/wind',\n",
    "           'wind': f'/mnt/data/sonia/cyclone/{grid}/wind_500hpa',\n",
    "           'temperature': f'/mnt/data/sonia/cyclone/{grid}/temperature',\n",
    "           'humidity': f'/mnt/data/sonia/cyclone/{grid}/humidity',\n",
    "           'topo': f'/mnt/data/sonia/cyclone/{grid}/slp/topo.nc'} # where the source data is stored \n",
    "varfuncs = {}\n",
    "climatology = {}\n",
    "if use_slp:\n",
    "    varnames.append('slp')\n",
    "    def f_slp(ds, lats, lons, time=None): # function to run when new SLP file is loaded\n",
    "        if time is None:\n",
    "            return ds.sel(lat=lats, lon=lons)['slp']\n",
    "        else:\n",
    "            return ds.sel(lat=lats, lon=lons, time=time)['slp']\n",
    "    varfuncs['slp'] = f_slp\n",
    "if use_windmag:\n",
    "    varnames.append('wind')\n",
    "    def f_wind(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            u = ds.sel(lat=lats, lon=lons)['u'] # for 10m: [['u10', 'v10']] \n",
    "            v = ds.sel(lat=lats, lon=lons)['v']\n",
    "        else:\n",
    "            u = ds.sel(lat=lats, lon=lons, time=time)['u'] # for 10m: [['u10', 'v10']] \n",
    "            v = ds.sel(lat=lats, lon=lons, time=time)['v']\n",
    "        windmag = np.sqrt(u**2 + v**2)\n",
    "        return windmag\n",
    "    varfuncs['wind'] = f_wind\n",
    "if use_winduv:\n",
    "    varnames.append('wind')\n",
    "    def f_winduv(ds, lats, lons, time=None):\n",
    "        # print(ds.sel(lat=lats, lon=lons))\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons)[['u', 'v']] # for 10m: [['u10', 'v10']] \n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time)[['u', 'v']] # for 10m: [['u10', 'v10']] \n",
    "        return data\n",
    "    varfuncs['wind'] = f_winduv\n",
    "if use_temperature:\n",
    "    varnames.append('temperature')\n",
    "    def f_temperature(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons, pressure_level=925)['t']\n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time, pressure_level=925)['t']\n",
    "        return data\n",
    "    varfuncs['temperature'] = f_temperature\n",
    "if use_humidity:\n",
    "    varnames.append('humidity')\n",
    "    def f_humidity(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons, pressure_level=500)['q']\n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time, pressure_level=500)['q']\n",
    "        return data \n",
    "    varfuncs['humidity'] = f_humidity\n",
    "topo = None\n",
    "if use_topo: \n",
    "    varnames.append('topo')\n",
    "    topo = xr.open_dataset(varlocs['topo'], engine='netcdf4')\n",
    "    def f_topo(ds, lats, lons, time=None):\n",
    "        return ds.sel(lat=lats, lon=lons)['lsm']\n",
    "varnames, varfuncs\n",
    "\n",
    "resolution = grid # resolution of data in degs (may later get redefined by climax checkpoint reso)\n",
    "l = 800 # (half length: l/2 km from center in each direction)\n",
    "s = 32 # box will be dimensions s by s (eg 32x32)\n",
    "x_lin = np.linspace(-l, l, s)\n",
    "y_lin = np.linspace(-l, l, s)\n",
    "x_grid, y_grid = np.meshgrid(x_lin, y_lin) # equal-spaced points from -l to l in both x and y dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76aea4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3519284/1857287072.py:6: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  correct_time = cur_data['time'].values[0] + pd.to_timedelta(np.arange(cur_data.dims['time']) * 6, unit='h')\n"
     ]
    }
   ],
   "source": [
    "file_year = 1940\n",
    "end_year = 2024\n",
    "cur_datas = {}\n",
    "for var in varnames:\n",
    "    cur_data = xr.open_dataset(f'{varlocs[var]}/{var}.{file_year}.nc', engine='netcdf4')\n",
    "    correct_time = cur_data['time'].values[0] + pd.to_timedelta(np.arange(cur_data.dims['time']) * 6, unit='h')\n",
    "    cur_data = cur_data.assign_coords(time=correct_time) # incase it wasn't read in as 6hrly\n",
    "    cur_datas[var] = cur_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdccd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "truetrain = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/train'))\n",
    "trueval = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/val'))\n",
    "truetest = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/test'))\n",
    "\n",
    "tracks['split'] = 0\n",
    "tracks.loc[tracks['sid'].isin(trueval), 'split'] = 1\n",
    "tracks.loc[tracks['sid'].isin(truetest), 'split'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bf75a",
   "metadata": {},
   "source": [
    "# The inverse\n",
    "From global climaX output to series of 32x32 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181dd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj \n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "predpath = '/home/sonia/climaxinf/out/date/natlantic-multivar-fullcontext/test'\n",
    "outpath = '/mnt/data/sonia/climax-data/date/extracted-natlantic-multivar-fullcontext/test'\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "resolution = 1.40625\n",
    "l = 800 # (half length: l/2 km from center in each direction)\n",
    "s = 32 # box will be dimensions s by s (eg 32x32)\n",
    "### HERE WE REINTRODUE THE N/S FLIP: ###\n",
    "x_lin = np.linspace(-l, l, s)\n",
    "y_lin = np.linspace(-l, l, s)\n",
    "x_grid, y_grid = np.meshgrid(x_lin, y_lin) # equal-spaced points from -l to l in both x and y dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a7b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 899/899 [01:47<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for fname in tqdm(os.listdir(predpath)):\n",
    "    worlds = np.load(os.path.join(predpath, fname))\n",
    "    if np.isnan(worlds).sum() > 0:\n",
    "        print('NANS', np.isnan(worlds).sum())\n",
    "    sid = fname[:-4]\n",
    "    records = tracks[tracks['sid']==sid].to_dict('records')\n",
    "    boxes = []\n",
    "        \n",
    "    for record, world in zip(records, worlds): # iterates over time\n",
    "        lats = np.linspace(-90, 90, 128)\n",
    "        lons = np.linspace(0, 360, 256, endpoint=False)\n",
    "        ds = xr.Dataset(\n",
    "            data_vars={'t2m': (('lat', 'lon'), world[0]), # EDIT THESE!\n",
    "                       'z': (('lat', 'lon'), world[1]),\n",
    "                       'u': (('lat', 'lon'), world[2]),\n",
    "                       'v': (('lat', 'lon'), world[3]),\n",
    "                       't': (('lat', 'lon'), world[4]),\n",
    "                       'q': (('lat', 'lon'), world[5])}, \n",
    "            coords={\"lat\": lats, \"lon\": lons}, # Attach the coordinates\n",
    "        )\n",
    "\n",
    "        lat_center, lon_center = record['lat'], record['lon']\n",
    "        proj_km = Proj(proj='aeqd', lat_0=lat_center, lon_0=lon_center, units='km')\n",
    "        lon_grid, lat_grid = proj_km(x_grid, y_grid, inverse=True) #translate km to deg\n",
    "        lon_grid=(lon_grid+360)%360 # because these datasets have lon as 0 to 360 (lat is still -90 to 90)\n",
    "        lon_min = lon_grid.min() - resolution # +- reso because otherwise xarray will not include the edge points\n",
    "        lon_max = lon_grid.max() + resolution\n",
    "        lat_min = lat_grid.min() - resolution\n",
    "        lat_max = lat_grid.max() + resolution\n",
    "\n",
    "        selection = ds.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))\n",
    "        lats = selection.lat.values \n",
    "        lons = selection.lon.values\n",
    "        data = selection.to_array().values\n",
    "\n",
    "        slices = []\n",
    "        if data.shape[0] > 1: # for instance, wind u and v components\n",
    "            for i in range(data.shape[0]):\n",
    "                # Build interpolator\n",
    "                interp = RegularGridInterpolator(\n",
    "                    (lats, lons),\n",
    "                    data[i],\n",
    "                    bounds_error=False,\n",
    "                    fill_value=None\n",
    "                )\n",
    "\n",
    "                # Interpolate at new (lat, lon) pairs\n",
    "                interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "                interp_values = interp(interp_points).reshape(s, s)\n",
    "                slices.append(interp_values)\n",
    "        else:\n",
    "            # Build interpolator\n",
    "            interp = RegularGridInterpolator(\n",
    "                (lats, lons),\n",
    "                data.squeeze(),\n",
    "                bounds_error=False,\n",
    "                fill_value=np.nan\n",
    "            )\n",
    "\n",
    "            # Interpolate at new (lat, lon) pairs\n",
    "            interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "            interp_values = interp(interp_points).reshape(s, s)\n",
    "            slices.append(interp_values)\n",
    "            \n",
    "        # windmag = np.sqrt(slices[0]**2 + slices[1]**2)\n",
    "        # boxes.append(windmag)\n",
    "        slp = 925 * np.exp(slices[1] / (287.05 * slices[0])) # Formula: P_slp = P_level * exp(Phi / (Rd * T))\n",
    "        frame = np.stack([slp]+slices[2:], axis=-1) # We want H x W x V\n",
    "        boxes.append(frame)\n",
    "\n",
    "    # result = np.stack(boxes, axis=0)\n",
    "    # np.save(os.path.join(outpath, f'{sid}.npy'), result)\n",
    "    os.makedirs(os.path.join(outpath, sid), exist_ok=True)\n",
    "    for i in range(len(boxes)):\n",
    "        np.save(os.path.join(outpath, sid, f'{i}.npy'), boxes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c83631f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5eeed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6, 128, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worlds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208d938",
   "metadata": {},
   "source": [
    "# Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "154cbabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/sonia/clima_patches/date/natlantic-humidity-0.25'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = f'/mnt/data/sonia/clima_patches/date/{basin}-humidity-0.25'\n",
    "climatology_path = '/mnt/data/sonia/cyclone/0.25/humidity/monthly_climatology.nc'\n",
    "clima = xr.open_dataset(climatology_path)\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "readme = f'based on climatology at {climatology_path}'\n",
    "with open(os.path.join(out_path, 'readme.txt'), 'w') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "testvalsids = tracks[tracks['split']>0]\n",
    "testvalframes = [group.to_dict('records') for _, group in testvalsids.groupby('sid')]\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33e9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_point_fulldata(frames): # provide actual climate data, not patch plus climatology\n",
    "    \"\"\"make one training datapoint. df contains year/../hr, lat, lon of center\"\"\"\n",
    "    # if skip_preexisting and frame['sid'] in exists:\n",
    "    #     return\n",
    "    boxes = []\n",
    "            \n",
    "    for frame in frames:\n",
    "        year, month, day, hour = frame['year'], frame['month'], frame['day'], frame['hour']\n",
    "        time = f'{year}-{month:02d}-{day:02d}T{hour:02d}:00:00'\n",
    "        cur_datas = clima.sel(month=month)\n",
    "        \n",
    "        lat_center, lon_center = frame['lat'], frame['lon']\n",
    "        # 'aeqd': https://proj.org/en/stable/operations/projections/aeqd.html\n",
    "        proj_km = Proj(proj='aeqd', lat_0=lat_center, lon_0=lon_center, units='km')\n",
    "        # Project to find lat/lon corners of the equal-area box\n",
    "        lon_grid, lat_grid = proj_km(x_grid, y_grid, inverse=True) #translate km to deg\n",
    "        lon_grid=(lon_grid+360)%360 # because these datasets have lon as 0 to 360 (lat is still -90 to 90)\n",
    "        lon_min = lon_grid.min() - resolution # +- reso because otherwise xarray will not include the edge points\n",
    "        lon_max = lon_grid.max() + resolution\n",
    "        lat_min = lat_grid.min() - resolution\n",
    "        lat_max = lat_grid.max() + resolution\n",
    "        \n",
    "        # for var in varnames:\n",
    "        data = varfuncs[var](cur_datas, slice(lat_max, lat_min), slice(lon_min, lon_max), time=None)\n",
    "        \n",
    "        lats = data.lat.values \n",
    "        lons = data.lon.values\n",
    "        \n",
    "        slices=[]\n",
    "        if type(data) == xr.Dataset: # for instance, wind u and v components (data.shape[0] or data.to_array().shape[0] ??)\n",
    "            data = data.to_array().values.squeeze()\n",
    "            # print(data.shape)\n",
    "            for i in range(data.shape[0]):\n",
    "                sel = data[i]\n",
    "                # Build interpolator\n",
    "                interp = RegularGridInterpolator(\n",
    "                    (lats, lons),\n",
    "                    sel,\n",
    "                    bounds_error=False,\n",
    "                    fill_value=np.nan\n",
    "                )\n",
    "                \n",
    "                # Interpolate at new (lat, lon) pairs\n",
    "                interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "                interp_values = interp(interp_points).reshape(s, s)\n",
    "                slices.append(interp_values)\n",
    "        else: # just one channel (eg slp)\n",
    "            data = np.asarray(data).squeeze()\n",
    "            # Build interpolator\n",
    "            interp = RegularGridInterpolator(\n",
    "                (lats, lons),\n",
    "                data,\n",
    "                bounds_error=False,\n",
    "                fill_value=np.nan\n",
    "            )\n",
    "            \n",
    "            # Interpolate at new (lat, lon) pairs\n",
    "            interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "            interp_values = interp(interp_points).reshape(s, s)\n",
    "            slices.append(interp_values)\n",
    "            \n",
    "        boxes.append(np.stack(slices, axis=-1).squeeze())\n",
    "        \n",
    "    # split = '' \n",
    "    if frame['split'] == 0:\n",
    "        split = 'train'\n",
    "    elif frame['split'] == 1:\n",
    "        split = 'val'\n",
    "    elif frame['split'] == 2:\n",
    "        split = 'test'\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected split value {frame['split']}\")\n",
    "    \n",
    "    os.makedirs(os.path.join(out_path, split, frame['sid']), exist_ok=True)\n",
    "    for i in range(len(boxes)):\n",
    "        np.save(os.path.join(out_path, split, f\"{frame['sid']}/{i}\"), boxes[i])\n",
    "        \n",
    "    return boxes\n",
    "    \n",
    "# prompt = prep_point(tracks.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbffffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 899/899 [00:08<00:00, 109.94it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(out_path, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_path, 'test'), exist_ok=True)\n",
    "\n",
    "for storm in tqdm(testvalframes):\n",
    "    boxes =prep_point_fulldata(storm[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5a7f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ae908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
