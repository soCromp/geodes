{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf2a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nts: activate svd \n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "import threading\n",
    "from pyproj import Proj \n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import random \n",
    "\n",
    "###### settings\n",
    "trackspath1='/home/sonia/mcms/tracker/1940-2010/era5/out_era5/era5/mcms_era5_1940_2010_tracks.txt'\n",
    "trackspath2='/home/sonia/mcms/tracker/2010-2024/era5/out_era5/era5/FIXEDmcms_era5_2010_2024_tracks.txt'\n",
    "joinyear = 2010 # overlap for the track data\n",
    "\n",
    "use_slp = False # whether to include slp channel\n",
    "use_windmag = False #include wind magnitude channel # NOTE THIS IS 500hPa\n",
    "use_winduv = False # include wind u and v components channels # NOTE THIS IS 500hPa\n",
    "use_temperature = False # temperature at 925hPa \n",
    "use_humidity = True # specific humidity at 500hPa\n",
    "use_topo = False # include topography channel\n",
    "skip_preexisting = False # skip existing datapoints (ensures they have 8 frames)\n",
    "threads = 1 # >1 not implemented\n",
    "\n",
    "# atlantic ocean is regmask['reg_name'].values[109] # so 110 in regmaskoc values\n",
    "# atlantic: 110\n",
    "# pacific: 111\n",
    "reg_id = 110\n",
    "hemi = 'n' # n or s\n",
    "###### \n",
    "\n",
    "if reg_id == 110:\n",
    "    basin = 'atlantic'\n",
    "elif reg_id == 111:\n",
    "    basin = 'pacific'\n",
    "basin = hemi + basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac4d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "regmask = xr.open_dataset('/home/cyclone/regmask_0723_anl.nc')\n",
    "\n",
    "####### make dataframe of all tracks \n",
    "tracks1 = pd.read_csv(trackspath1, sep=' ', header=None, \n",
    "        names=['year', 'month', 'day', 'hour', 'total_hrs', 'unk1', 'unk2', 'unk3', 'unk4', 'unk5', 'unk6', \n",
    "               'z1', 'z2', 'unk7', 'tid', 'sid'])\n",
    "# storms that start before the join year (even if they continue into the join year):\n",
    "sids1 = tracks1[(tracks1['sid']==tracks1['tid']) & (tracks1['year']<joinyear)]['sid'].unique()\n",
    "tracks1 = tracks1[tracks1['sid'].isin(sids1)]\n",
    "\n",
    "tracks2 = pd.read_csv(trackspath2, sep=' ', header=None, \n",
    "        names=['year', 'month', 'day', 'hour', 'total_hrs', 'unk1', 'unk2', 'unk3', 'unk4', 'unk5', 'unk6', \n",
    "               'z1', 'z2', 'unk7', 'tid', 'sid'])\n",
    "# filter out storms that \"start\" at the beginning of the join year since they probably started before and are \n",
    "# included in tracks1\n",
    "sids2 = tracks2[(tracks2['sid']==tracks2['tid']) & \\\n",
    "        ((tracks2['year']>=joinyear) | (tracks2['month']>1) | (tracks2['day']>1) | (tracks2['hour']>0))]['sid'].unique()\n",
    "tracks2 = tracks2[tracks2['sid'].isin(sids2)]\n",
    "\n",
    "tracks = pd.concat([tracks1, tracks2], ignore_index=True)\n",
    "tracks = tracks.sort_values(by=['year', 'month', 'day', 'hour'])\n",
    "\n",
    "# conversions from the MCMS lat/lon system, as described in Jimmy's email:\n",
    "tracks['lat'] = 90-tracks['unk1'].values/100\n",
    "tracks['lon'] = tracks['unk2'].values/100\n",
    "\n",
    "tracks = tracks[['year', 'month', 'day', 'hour', 'tid', 'sid', 'lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edf28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### variables prep\n",
    "grid = 0.25\n",
    "varnames = [] # list of variables that will be included in this output dataset\n",
    "varlocs = {'slp': f'/mnt/data/sonia/cyclone/{grid}/slp', #'wind10m': '/home/cyclone/wind',\n",
    "           'wind': f'/mnt/data/sonia/cyclone/{grid}/wind_500hpa',\n",
    "           'temperature': f'/mnt/data/sonia/cyclone/{grid}/temperature',\n",
    "           'humidity': f'/mnt/data/sonia/cyclone/{grid}/humidity',\n",
    "           'topo': f'/mnt/data/sonia/cyclone/{grid}/slp/topo.nc'} # where the source data is stored \n",
    "varfuncs = {}\n",
    "climatology = {}\n",
    "if use_slp:\n",
    "    varnames.append('slp')\n",
    "    def f_slp(ds, lats, lons, time=None): # function to run when new SLP file is loaded\n",
    "        if time is None:\n",
    "            return ds.sel(lat=lats, lon=lons)['slp']\n",
    "        else:\n",
    "            return ds.sel(lat=lats, lon=lons, time=time)['slp']\n",
    "    varfuncs['slp'] = f_slp\n",
    "if use_windmag:\n",
    "    varnames.append('wind')\n",
    "    def f_wind(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            u = ds.sel(lat=lats, lon=lons)['u'] # for 10m: [['u10', 'v10']] \n",
    "            v = ds.sel(lat=lats, lon=lons)['v']\n",
    "        else:\n",
    "            u = ds.sel(lat=lats, lon=lons, time=time)['u'] # for 10m: [['u10', 'v10']] \n",
    "            v = ds.sel(lat=lats, lon=lons, time=time)['v']\n",
    "        windmag = np.sqrt(u**2 + v**2)\n",
    "        return windmag\n",
    "    varfuncs['wind'] = f_wind\n",
    "if use_winduv:\n",
    "    varnames.append('wind')\n",
    "    def f_winduv(ds, lats, lons, time=None):\n",
    "        # print(ds.sel(lat=lats, lon=lons))\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons)[['u', 'v']] # for 10m: [['u10', 'v10']] \n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time)[['u', 'v']] # for 10m: [['u10', 'v10']] \n",
    "        return data\n",
    "    varfuncs['wind'] = f_winduv\n",
    "if use_temperature:\n",
    "    varnames.append('temperature')\n",
    "    def f_temperature(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons, pressure_level=925)['t']\n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time, pressure_level=925)['t']\n",
    "        return data\n",
    "    varfuncs['temperature'] = f_temperature\n",
    "if use_humidity:\n",
    "    varnames.append('humidity')\n",
    "    def f_humidity(ds, lats, lons, time=None):\n",
    "        if time is None:\n",
    "            data = ds.sel(lat=lats, lon=lons, pressure_level=500)['q']\n",
    "        else:\n",
    "            data = ds.sel(lat=lats, lon=lons, time=time, pressure_level=500)['q']\n",
    "        return data \n",
    "    varfuncs['humidity'] = f_humidity\n",
    "topo = None\n",
    "if use_topo: \n",
    "    varnames.append('topo')\n",
    "    topo = xr.open_dataset(varlocs['topo'], engine='netcdf4')\n",
    "    def f_topo(ds, lats, lons, time=None):\n",
    "        return ds.sel(lat=lats, lon=lons)['lsm']\n",
    "varnames, varfuncs\n",
    "\n",
    "resolution = grid # resolution of data in degs (may later get redefined by climax checkpoint reso)\n",
    "l = 800 # (half length: l/2 km from center in each direction)\n",
    "s = 32 # box will be dimensions s by s (eg 32x32)\n",
    "x_lin = np.linspace(-l, l, s)\n",
    "y_lin = np.linspace(-l, l, s)\n",
    "x_grid, y_grid = np.meshgrid(x_lin, y_lin) # equal-spaced points from -l to l in both x and y dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76aea4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3509908/1857287072.py:6: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  correct_time = cur_data['time'].values[0] + pd.to_timedelta(np.arange(cur_data.dims['time']) * 6, unit='h')\n"
     ]
    }
   ],
   "source": [
    "file_year = 1940\n",
    "end_year = 2024\n",
    "cur_datas = {}\n",
    "for var in varnames:\n",
    "    cur_data = xr.open_dataset(f'{varlocs[var]}/{var}.{file_year}.nc', engine='netcdf4')\n",
    "    correct_time = cur_data['time'].values[0] + pd.to_timedelta(np.arange(cur_data.dims['time']) * 6, unit='h')\n",
    "    cur_data = cur_data.assign_coords(time=correct_time) # incase it wasn't read in as 6hrly\n",
    "    cur_datas[var] = cur_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdccd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "truetrain = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/train'))\n",
    "trueval = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/val'))\n",
    "truetest = set(os.listdir(f'/home/cyclone/train/windmag/500hpa/0.25/date/{basin}/test'))\n",
    "\n",
    "tracks['split'] = 0\n",
    "tracks.loc[tracks['sid'].isin(trueval), 'split'] = 1\n",
    "tracks.loc[tracks['sid'].isin(truetest), 'split'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b0d78",
   "metadata": {},
   "source": [
    "# Forwards\n",
    "Create climaX prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f3157a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '/mnt/data/sonia/climax-data/date/natlantic-windmaguv10m-fullcontext'\n",
    "\n",
    "init_frames = tracks[(tracks['sid']==tracks['tid']) & (tracks['split']>0)].to_dict('records')\n",
    "len(init_frames)\n",
    "\n",
    "try:\n",
    "    exists = set(os.listdir(os.path.join(outpath, 'val')) + \\\n",
    "        os.listdir(os.path.join(outpath, 'test')))\n",
    "except:\n",
    "    exists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0232ab72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176909eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_point_fulldata(frame): # provide actual climate data, not patch plus climatology\n",
    "    \"\"\"make one training datapoint. df contains year/../hr, lat, lon of center\"\"\"\n",
    "    if skip_preexisting and frame['sid'] in exists:\n",
    "        return\n",
    "    boxes = []\n",
    "    global file_year\n",
    "    if frame['year'] != file_year: # starts in next year, so we know no following storm will start in cur year\n",
    "        file_year = frame['year'] # advance one year (or more if there were no storms in this year / we skip already processed points)\n",
    "        for var in varnames:\n",
    "            next_data = xr.open_dataset(f'{varlocs[var]}/{var}.{file_year}.nc', engine='netcdf4')\n",
    "            correct_time = next_data['time'].values[0] + pd.to_timedelta(np.arange(next_data.dims['time']) * 6, unit='h')\n",
    "            next_data = next_data.assign_coords(time=correct_time) # incase it wasn't read in as 6hrly\n",
    "            cur_datas[var] = next_data\n",
    "            \n",
    "    year, month, day, hour = frame['year'], frame['month'], frame['day'], frame['hour']\n",
    "    time = f'{year}-{month:02d}-{day:02d}T{hour:02d}:00:00'\n",
    "    \n",
    "    for var in varnames:\n",
    "        data = varfuncs[var](cur_datas[var], \n",
    "                             slice(90,-90), \n",
    "                             slice(0,360), time)\n",
    "    if len(varnames)>1:\n",
    "        data = xr.merge(data_vars)\n",
    "\n",
    "    new_lat = np.linspace(90,-90, 128) # north to south\n",
    "    new_lon = np.linspace(0, 359.5, 256)\n",
    "    data_resized = data.interp(lat=new_lat, lon=new_lon, method=\"linear\")\n",
    "        \n",
    "    result = data_resized.to_array(dim=\"variable\").values.squeeze()\n",
    "    if frame['split'] == 0:\n",
    "        np.save(os.path.join(outpath, 'train', f\"{frame['sid']}.np\"), result)\n",
    "    elif frame['split'] == 1:\n",
    "        np.save(os.path.join(outpath, 'val', f\"{frame['sid']}.np\"), result)\n",
    "    elif frame['split'] == 2:\n",
    "        np.save(os.path.join(outpath, 'test', f\"{frame['sid']}.np\"), result)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected split value {frame['split']}\")\n",
    "    \n",
    "# prompt = prep_point(tracks.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15df019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1280 [00:00<?, ?it/s]/tmp/ipykernel_682780/1420768760.py:11: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  correct_time = next_data['time'].values[0] + pd.to_timedelta(np.arange(next_data.dims['time']) * 6, unit='h')\n",
      "  0%|          | 2/1280 [00:14<2:29:11,  7.00s/it]/tmp/ipykernel_682780/1420768760.py:11: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  correct_time = next_data['time'].values[0] + pd.to_timedelta(np.arange(next_data.dims['time']) * 6, unit='h')\n",
      "  0%|          | 6/1280 [00:50<2:57:50,  8.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m tqdm(init_frames):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m frame[\u001b[33m'\u001b[39m\u001b[33msid\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exists:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         \u001b[43mprep_point_fulldata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mprep_point_fulldata\u001b[39m\u001b[34m(frame)\u001b[39m\n\u001b[32m     24\u001b[39m new_lat = np.linspace(-\u001b[32m90\u001b[39m,\u001b[32m90\u001b[39m, \u001b[32m128\u001b[39m)\n\u001b[32m     25\u001b[39m new_lon = np.linspace(\u001b[32m0\u001b[39m, \u001b[32m359.5\u001b[39m, \u001b[32m256\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m data_resized = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_lat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_lon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m result = data_resized.to_array(dim=\u001b[33m\"\u001b[39m\u001b[33mvariable\u001b[39m\u001b[33m\"\u001b[39m).values.squeeze()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame[\u001b[33m'\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/dataset.py:3852\u001b[39m, in \u001b[36mDataset.interp\u001b[39m\u001b[34m(self, coords, method, assume_sorted, kwargs, method_non_numeric, **coords_kwargs)\u001b[39m\n\u001b[32m   3849\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype_kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muifc\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3850\u001b[39m     \u001b[38;5;66;03m# For normal number types do the interpolation:\u001b[39;00m\n\u001b[32m   3851\u001b[39m     var_indexers = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m use_indexers.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m var.dims}\n\u001b[32m-> \u001b[39m\u001b[32m3852\u001b[39m     variables[name] = \u001b[43mmissing\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3853\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype_kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mObU\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (use_indexers.keys() & var.dims):\n\u001b[32m   3854\u001b[39m     \u001b[38;5;66;03m# For types that we do not understand do stepwise\u001b[39;00m\n\u001b[32m   3855\u001b[39m     \u001b[38;5;66;03m# interpolation to avoid modifying the elements.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3858\u001b[39m     \u001b[38;5;66;03m# this loop there might be some duplicate code that slows it\u001b[39;00m\n\u001b[32m   3859\u001b[39m     \u001b[38;5;66;03m# down, therefore collect these signals and run it later:\u001b[39;00m\n\u001b[32m   3860\u001b[39m     reindex_vars.append(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/missing.py:667\u001b[39m, in \u001b[36minterp\u001b[39m\u001b[34m(var, indexes_coords, method, **kwargs)\u001b[39m\n\u001b[32m    665\u001b[39m broadcast_dims = [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m var.dims \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dims]\n\u001b[32m    666\u001b[39m original_dims = broadcast_dims + dims\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m result = \u001b[43minterpolate_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43moriginal_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mindep_indexes_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# dimension of the output array\u001b[39;00m\n\u001b[32m    675\u001b[39m out_dims: OrderedSet = OrderedSet()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/missing.py:740\u001b[39m, in \u001b[36minterpolate_variable\u001b[39m\u001b[34m(var, indexes_coords, method, kwargs)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;66;03m# scipy.interpolate.interp1d always forces to float.\u001b[39;00m\n\u001b[32m    739\u001b[39m dtype = \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(var.dtype.type, np.inexact) \u001b[38;5;28;01melse\u001b[39;00m var.dtype\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m result = \u001b[43mapply_ufunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_interpnd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mresult_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_core_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_core_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_core_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_core_dims\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_in_core_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallelized\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterp_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterp_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# we leave broadcasting up to dask if possible\u001b[39;49;00m\n\u001b[32m    753\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# but we need broadcasted values in _interpnd, so propagate that\u001b[39;49;00m\n\u001b[32m    754\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# context (dimension names), and broadcast there\u001b[39;49;00m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This would be unnecessary if we could tell apply_ufunc\u001b[39;49;00m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# to insert size-1 broadcast dimensions\u001b[39;49;00m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult_coord_core_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_core_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult_coords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO: deprecate and have the user rechunk themselves\u001b[39;49;00m\n\u001b[32m    760\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdask_gufunc_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_rechunk\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvectorize_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/computation/apply_ufunc.py:1272\u001b[39m, in \u001b[36mapply_ufunc\u001b[39m\u001b[34m(func, input_core_dims, output_core_dims, exclude_dims, vectorize, join, dataset_join, dataset_fill_value, keep_attrs, kwargs, dask, output_dtypes, output_sizes, meta, dask_gufunc_kwargs, on_missing_core_dim, *args)\u001b[39m\n\u001b[32m   1270\u001b[39m \u001b[38;5;66;03m# feed Variables directly through apply_variable_ufunc\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(a, Variable) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[32m-> \u001b[39m\u001b[32m1272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvariables_vfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1274\u001b[39m     \u001b[38;5;66;03m# feed anything else through apply_array_ufunc\u001b[39;00m\n\u001b[32m   1275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_array_ufunc(func, *args, dask=dask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/computation/apply_ufunc.py:723\u001b[39m, in \u001b[36mapply_variable_ufunc\u001b[39m\u001b[34m(func, signature, exclude_dims, dask, output_dtypes, vectorize, keep_attrs, dask_gufunc_kwargs, *args)\u001b[39m\n\u001b[32m    718\u001b[39m broadcast_dims = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    719\u001b[39m     dim \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m dim_sizes \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m signature.all_core_dims\n\u001b[32m    720\u001b[39m )\n\u001b[32m    721\u001b[39m output_dims = [broadcast_dims + out \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m signature.output_core_dims]\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m input_data = \u001b[43m[\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbroadcast_compat_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbroadcast_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVariable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_dims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_core_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(is_chunked_array(array) \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m input_data):\n\u001b[32m    733\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dask == \u001b[33m\"\u001b[39m\u001b[33mforbidden\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/computation/apply_ufunc.py:725\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    718\u001b[39m broadcast_dims = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    719\u001b[39m     dim \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m dim_sizes \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m signature.all_core_dims\n\u001b[32m    720\u001b[39m )\n\u001b[32m    721\u001b[39m output_dims = [broadcast_dims + out \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m signature.output_core_dims]\n\u001b[32m    723\u001b[39m input_data = [\n\u001b[32m    724\u001b[39m     (\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m         \u001b[43mbroadcast_compat_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbroadcast_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Variable)\n\u001b[32m    727\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[32m    728\u001b[39m     )\n\u001b[32m    729\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m arg, core_dims \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, signature.input_core_dims, strict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    730\u001b[39m ]\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(is_chunked_array(array) \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m input_data):\n\u001b[32m    733\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dask == \u001b[33m\"\u001b[39m\u001b[33mforbidden\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/computation/apply_ufunc.py:646\u001b[39m, in \u001b[36mbroadcast_compat_data\u001b[39m\u001b[34m(variable, broadcast_dims, core_dims)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbroadcast_compat_data\u001b[39m(\n\u001b[32m    642\u001b[39m     variable: Variable,\n\u001b[32m    643\u001b[39m     broadcast_dims: \u001b[38;5;28mtuple\u001b[39m[Hashable, ...],\n\u001b[32m    644\u001b[39m     core_dims: \u001b[38;5;28mtuple\u001b[39m[Hashable, ...],\n\u001b[32m    645\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     data = \u001b[43mvariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m    648\u001b[39m     old_dims = variable.dims\n\u001b[32m    649\u001b[39m     new_dims = broadcast_dims + core_dims\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/variable.py:430\u001b[39m, in \u001b[36mVariable.data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m     duck_array = \u001b[38;5;28mself\u001b[39m._data.array\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._data, indexing.ExplicitlyIndexed):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     duck_array = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_duck_array(\u001b[38;5;28mself\u001b[39m._data):\n\u001b[32m    432\u001b[39m     duck_array = \u001b[38;5;28mself\u001b[39m._data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/indexing.py:730\u001b[39m, in \u001b[36mLazilyVectorizedIndexedArray.get_duck_array\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    726\u001b[39m     array = apply_indexer(\u001b[38;5;28mself\u001b[39m.array, \u001b[38;5;28mself\u001b[39m.key)\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    728\u001b[39m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[32m    729\u001b[39m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     array = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[32m    732\u001b[39m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[32m    733\u001b[39m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[32m    734\u001b[39m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:108\u001b[39m, in \u001b[36mNetCDF4ArrayWrapper.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexingSupport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/core/indexing.py:1023\u001b[39m, in \u001b[36mexplicit_indexing_adapter\u001b[39m\u001b[34m(key, shape, indexing_support, raw_indexing_method)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[32m   1002\u001b[39m \n\u001b[32m   1003\u001b[39m \u001b[33;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1020\u001b[39m \u001b[33;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1022\u001b[39m raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m result = \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices.tuple:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# index the loaded duck array\u001b[39;00m\n\u001b[32m   1026\u001b[39m     indexable = as_indexable(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/svd/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:121\u001b[39m, in \u001b[36mNetCDF4ArrayWrapper._getitem\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.datastore.lock:\n\u001b[32m    120\u001b[39m         original_array = \u001b[38;5;28mself\u001b[39m.get_array(needs_lock=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         array = getitem(original_array, key)\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# indexer is used before the data is loaded from disk.\u001b[39;00m\n\u001b[32m    126\u001b[39m     msg = (\n\u001b[32m    127\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe indexing operation you are attempting to perform \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mis not valid on netCDF4.Variable object. Try loading \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myour data into memory first by calling .load().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(outpath, exist_ok=True)\n",
    "os.makedirs(os.path.join(outpath, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(outpath, 'test'), exist_ok=True)\n",
    "\n",
    "for frame in tqdm(init_frames):\n",
    "    if frame['sid'] not in exists:\n",
    "        prep_point_fulldata(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bf75a",
   "metadata": {},
   "source": [
    "# The inverse\n",
    "From global climaX output to series of 32x32 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181dd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj \n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "predpath = '/home/sonia/climaxinf/out/date/natlantic-multivar-fullcontext/test'\n",
    "outpath = '/mnt/data/sonia/climax-data/date/extracted-natlantic-multivar-fullcontext/test'\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "resolution = 1.40625\n",
    "l = 800 # (half length: l/2 km from center in each direction)\n",
    "s = 32 # box will be dimensions s by s (eg 32x32)\n",
    "### HERE WE REINTRODUE THE N/S FLIP: ###\n",
    "x_lin = np.linspace(-l, l, s)\n",
    "y_lin = np.linspace(-l, l, s)\n",
    "x_grid, y_grid = np.meshgrid(x_lin, y_lin) # equal-spaced points from -l to l in both x and y dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12a7b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 899/899 [01:47<00:00,  8.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for fname in tqdm(os.listdir(predpath)):\n",
    "    worlds = np.load(os.path.join(predpath, fname))\n",
    "    if np.isnan(worlds).sum() > 0:\n",
    "        print('NANS', np.isnan(worlds).sum())\n",
    "    sid = fname[:-4]\n",
    "    records = tracks[tracks['sid']==sid].to_dict('records')\n",
    "    boxes = []\n",
    "        \n",
    "    for record, world in zip(records, worlds): # iterates over time\n",
    "        lats = np.linspace(-90, 90, 128)\n",
    "        lons = np.linspace(0, 360, 256, endpoint=False)\n",
    "        ds = xr.Dataset(\n",
    "            data_vars={'t2m': (('lat', 'lon'), world[0]), # EDIT THESE!\n",
    "                       'z': (('lat', 'lon'), world[1]),\n",
    "                       'u': (('lat', 'lon'), world[2]),\n",
    "                       'v': (('lat', 'lon'), world[3]),\n",
    "                       't': (('lat', 'lon'), world[4]),\n",
    "                       'q': (('lat', 'lon'), world[5])}, \n",
    "            coords={\"lat\": lats, \"lon\": lons}, # Attach the coordinates\n",
    "        )\n",
    "\n",
    "        lat_center, lon_center = record['lat'], record['lon']\n",
    "        proj_km = Proj(proj='aeqd', lat_0=lat_center, lon_0=lon_center, units='km')\n",
    "        lon_grid, lat_grid = proj_km(x_grid, y_grid, inverse=True) #translate km to deg\n",
    "        lon_grid=(lon_grid+360)%360 # because these datasets have lon as 0 to 360 (lat is still -90 to 90)\n",
    "        lon_min = lon_grid.min() - resolution # +- reso because otherwise xarray will not include the edge points\n",
    "        lon_max = lon_grid.max() + resolution\n",
    "        lat_min = lat_grid.min() - resolution\n",
    "        lat_max = lat_grid.max() + resolution\n",
    "\n",
    "        selection = ds.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))\n",
    "        lats = selection.lat.values \n",
    "        lons = selection.lon.values\n",
    "        data = selection.to_array().values\n",
    "\n",
    "        slices = []\n",
    "        if data.shape[0] > 1: # for instance, wind u and v components\n",
    "            for i in range(data.shape[0]):\n",
    "                # Build interpolator\n",
    "                interp = RegularGridInterpolator(\n",
    "                    (lats, lons),\n",
    "                    data[i],\n",
    "                    bounds_error=False,\n",
    "                    fill_value=None\n",
    "                )\n",
    "\n",
    "                # Interpolate at new (lat, lon) pairs\n",
    "                interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "                interp_values = interp(interp_points).reshape(s, s)\n",
    "                slices.append(interp_values)\n",
    "        else:\n",
    "            # Build interpolator\n",
    "            interp = RegularGridInterpolator(\n",
    "                (lats, lons),\n",
    "                data.squeeze(),\n",
    "                bounds_error=False,\n",
    "                fill_value=np.nan\n",
    "            )\n",
    "\n",
    "            # Interpolate at new (lat, lon) pairs\n",
    "            interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "            interp_values = interp(interp_points).reshape(s, s)\n",
    "            slices.append(interp_values)\n",
    "            \n",
    "        # windmag = np.sqrt(slices[0]**2 + slices[1]**2)\n",
    "        # boxes.append(windmag)\n",
    "        slp = 925 * np.exp(slices[1] / (287.05 * slices[0])) # Formula: P_slp = P_level * exp(Phi / (Rd * T))\n",
    "        frame = np.stack([slp]+slices[2:], axis=-1) # We want H x W x V\n",
    "        boxes.append(frame)\n",
    "\n",
    "    # result = np.stack(boxes, axis=0)\n",
    "    # np.save(os.path.join(outpath, f'{sid}.npy'), result)\n",
    "    os.makedirs(os.path.join(outpath, sid), exist_ok=True)\n",
    "    for i in range(len(boxes)):\n",
    "        np.save(os.path.join(outpath, sid, f'{i}.npy'), boxes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c83631f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5eeed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6, 128, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worlds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208d938",
   "metadata": {},
   "source": [
    "# Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "154cbabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/sonia/clima_patches/date/natlantic-humidity-0.25'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = f'/mnt/data/sonia/clima_patches/date/{basin}-humidity-0.25'\n",
    "climatology_path = '/mnt/data/sonia/cyclone/0.25/humidity/monthly_climatology.nc'\n",
    "clima = xr.open_dataset(climatology_path)\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "readme = f'based on climatology at {climatology_path}'\n",
    "with open(os.path.join(out_path, 'readme.txt'), 'w') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "testvalsids = tracks[tracks['split']>0]\n",
    "testvalframes = [group.to_dict('records') for _, group in testvalsids.groupby('sid')]\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33e9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_point_fulldata(frames): # provide actual climate data, not patch plus climatology\n",
    "    \"\"\"make one training datapoint. df contains year/../hr, lat, lon of center\"\"\"\n",
    "    # if skip_preexisting and frame['sid'] in exists:\n",
    "    #     return\n",
    "    boxes = []\n",
    "            \n",
    "    for frame in frames:\n",
    "        year, month, day, hour = frame['year'], frame['month'], frame['day'], frame['hour']\n",
    "        time = f'{year}-{month:02d}-{day:02d}T{hour:02d}:00:00'\n",
    "        cur_datas = clima.sel(month=month)\n",
    "        \n",
    "        lat_center, lon_center = frame['lat'], frame['lon']\n",
    "        # 'aeqd': https://proj.org/en/stable/operations/projections/aeqd.html\n",
    "        proj_km = Proj(proj='aeqd', lat_0=lat_center, lon_0=lon_center, units='km')\n",
    "        # Project to find lat/lon corners of the equal-area box\n",
    "        lon_grid, lat_grid = proj_km(x_grid, y_grid, inverse=True) #translate km to deg\n",
    "        lon_grid=(lon_grid+360)%360 # because these datasets have lon as 0 to 360 (lat is still -90 to 90)\n",
    "        lon_min = lon_grid.min() - resolution # +- reso because otherwise xarray will not include the edge points\n",
    "        lon_max = lon_grid.max() + resolution\n",
    "        lat_min = lat_grid.min() - resolution\n",
    "        lat_max = lat_grid.max() + resolution\n",
    "        \n",
    "        # for var in varnames:\n",
    "        data = varfuncs[var](cur_datas, slice(lat_max, lat_min), slice(lon_min, lon_max), time=None)\n",
    "        \n",
    "        lats = data.lat.values \n",
    "        lons = data.lon.values\n",
    "        \n",
    "        slices=[]\n",
    "        if type(data) == xr.Dataset: # for instance, wind u and v components (data.shape[0] or data.to_array().shape[0] ??)\n",
    "            data = data.to_array().values.squeeze()\n",
    "            # print(data.shape)\n",
    "            for i in range(data.shape[0]):\n",
    "                sel = data[i]\n",
    "                # Build interpolator\n",
    "                interp = RegularGridInterpolator(\n",
    "                    (lats, lons),\n",
    "                    sel,\n",
    "                    bounds_error=False,\n",
    "                    fill_value=np.nan\n",
    "                )\n",
    "                \n",
    "                # Interpolate at new (lat, lon) pairs\n",
    "                interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "                interp_values = interp(interp_points).reshape(s, s)\n",
    "                slices.append(interp_values)\n",
    "        else: # just one channel (eg slp)\n",
    "            data = np.asarray(data).squeeze()\n",
    "            # Build interpolator\n",
    "            interp = RegularGridInterpolator(\n",
    "                (lats, lons),\n",
    "                data,\n",
    "                bounds_error=False,\n",
    "                fill_value=np.nan\n",
    "            )\n",
    "            \n",
    "            # Interpolate at new (lat, lon) pairs\n",
    "            interp_points = np.stack([lat_grid.ravel(), lon_grid.ravel()], axis=-1)\n",
    "            interp_values = interp(interp_points).reshape(s, s)\n",
    "            slices.append(interp_values)\n",
    "            \n",
    "        boxes.append(np.stack(slices, axis=-1).squeeze())\n",
    "        \n",
    "    # split = '' \n",
    "    if frame['split'] == 0:\n",
    "        split = 'train'\n",
    "    elif frame['split'] == 1:\n",
    "        split = 'val'\n",
    "    elif frame['split'] == 2:\n",
    "        split = 'test'\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected split value {frame['split']}\")\n",
    "    \n",
    "    os.makedirs(os.path.join(out_path, split, frame['sid']), exist_ok=True)\n",
    "    for i in range(len(boxes)):\n",
    "        np.save(os.path.join(out_path, split, f\"{frame['sid']}/{i}\"), boxes[i])\n",
    "        \n",
    "    return boxes\n",
    "    \n",
    "# prompt = prep_point(tracks.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbffffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 899/899 [00:08<00:00, 109.94it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(out_path, 'val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_path, 'test'), exist_ok=True)\n",
    "\n",
    "for storm in tqdm(testvalframes):\n",
    "    boxes =prep_point_fulldata(storm[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5a7f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ae908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
